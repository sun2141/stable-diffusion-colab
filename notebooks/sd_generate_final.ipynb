{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì„¤ì¹˜ & ëŸ°íƒ€ì„ ìë™ ì¬ì‹œì‘\n",
    "!pip install --quiet \\\n",
    "  diffusers==0.26.3 \\\n",
    "  transformers==4.39.3 \\\n",
    "  huggingface_hub==0.25.1 \\\n",
    "  accelerate==0.29.2 \\\n",
    "  gradio==3.50.2 \\\n",
    "  peft==0.10.0 \\\n",
    "  safetensors torch torchvision torchaudio numpy scipy\n",
    "\n",
    "import os, IPython\n",
    "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ. ëŸ°íƒ€ì„ì„ ìë™ìœ¼ë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "os.kill(os.getpid(), 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Hugging Face ë¡œê·¸ì¸\n",
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e342fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ëª¨ë¸ ë¡œë”©\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "import torch\n",
    "\n",
    "pipe_txt2img = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def infer(prompt, init_image=None, strength=0.75):\n",
    "    if init_image is not None:\n",
    "        if isinstance(init_image, np.ndarray):\n",
    "            init_image = Image.fromarray(init_image)\n",
    "        elif not isinstance(init_image, Image.Image):\n",
    "            raise ValueError(f\"Unsupported image type: {type(init_image)}\")\n",
    "        init_image = init_image.resize((512, 512))\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            result = pipe_img2img(\n",
    "                prompt=[prompt],\n",
    "                image=init_image,\n",
    "                strength=strength,\n",
    "                guidance_scale=7.5\n",
    "            )\n",
    "    else:\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            result = pipe_txt2img(\n",
    "                prompt=[prompt],\n",
    "                guidance_scale=7.5\n",
    "            )\n",
    "    return result.images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c820eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸ¨ Stable Diffusion Image Generator\")\n",
    "\n",
    "    prompt_input = gr.Textbox(label=\"í”„ë¡¬í”„íŠ¸\", value=\"a cushion on a car seat\")\n",
    "    init_image = gr.Image(label=\"ì´ˆê¸° ì´ë¯¸ì§€ (ì„ íƒ)\", type=\"numpy\")\n",
    "    strength = gr.Slider(label=\"ìœ ì§€ ê°•ë„\", minimum=0.0, maximum=1.0, value=0.75)\n",
    "    gallery = gr.Gallery(label=\"ê²°ê³¼ ì´ë¯¸ì§€\")\n",
    "\n",
    "    run_button = gr.Button(\"ì´ë¯¸ì§€ ìƒì„±\")\n",
    "\n",
    "    run_button.click(fn=infer, inputs=[prompt_input, init_image, strength], outputs=[gallery])\n",
    "\n",
    "demo.launch(debug=True, share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
